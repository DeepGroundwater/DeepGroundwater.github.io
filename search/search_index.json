{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DeepGroundwater","text":""},{"location":"#promoting-research-through-blog-posts-and-open-source-code","title":"Promoting research through blog posts and open-source code","text":"<p>DeepGroundwater was inspired from the idea that not all research belongs on academic journals. There is great work done by students, postdocs, and research leads that may not advance the science, but deserves to be shared with the community. Furthermore, the peer-review process often can take weeks, if not months, for a paper to make it from draft to published. There is a need for a site to share developing reserach, and code, with others to spurn discovery and novelty; and we at Deep Groundwater hope we can provide that here. </p>"},{"location":"#editorial-board","title":"Editorial Board","text":"<p>DeepGroundwater is managed by the following maintainers:</p> <ul> <li>Tadd Bindas</li> <li>Jonathan M. Frame</li> <li>Ryoko Araki</li> <li>Jeremy Rapp</li> <li>Soelem Aafnan Bhuiyan</li> </ul>"},{"location":"#contributions","title":"Contributions","text":"<p>We welcome contributions from students, professors and industry professionals in the broad range of research related to hydrology and water resources. Contributions should be made by a GitHub pull request. An editorial review is required from at least two members of the board. </p> <p>Note that our editorial review is intended to ensure that the contribution is on brand for DeepGroundWater and the markdown formatting works with MkDocs, and is not intended to ensure scientific rigor.</p>"},{"location":"blog/the-technical-side-of-agu-2024-what-happened-and-where-are-we-going/","title":"The Technical side of AGU 2024: What happened and where are we going","text":"<p>Two weeks ago, the American Geophysical Union (AGU) hosted its annual fall meeting in Washington, D.C., with over 25,000 attendees from 100+ countries present to share their research. For those reading who have not been, nor heard of AGU, there are four major themes present:</p> <ul> <li>Earth's subsurface</li> <li>Earth's surface</li> <li>The atmosphere</li> <li>Space</li> </ul> <p>Among these four themes, there are several sections, and within each section there are many sessions corresponding to a research topic proposed by a group of scientists. Generally, most scientists submit one abstract to their field of study, and rarely, a second to a different section. At the conference research conversations occurred at posters, sessions, and oddly timed coffee hours during the lulls in programming (had to get my yearly zinger at AGU's coffee policy). Now that my brain, and feet from the 20,000 daily steps, have recovered, I want to write about my most significant takeaway from the week and where I predict things will be headed next year. </p>"},{"location":"blog/the-technical-side-of-agu-2024-what-happened-and-where-are-we-going/#authors-note","title":"Author's Note:","text":"<p>For context on my research at the conference, I spent the majority of my week attending sessions in the Atomospheric Sciences, Hydrology, Informatics, and Natural Hazards sections as my work/research involves hydrologic-focused machine learning methods and river routing (spending 80% of my time in Hydrology). I had two eLightning talks about representing reservoirs within a differentiable muskingum-cunge river routing model, and the Replace and Route NOAA-OWP application.</p>"},{"location":"blog/the-technical-side-of-agu-2024-what-happened-and-where-are-we-going/#welcome-to-open-access-30","title":"Welcome to Open-Access 3.0","text":"<p>My big takeaway was not that AI would be taking our jobs or that there was a new \"state-of-the-art\" model but that we're entering the next generation of data sharing for model inputs or validation. I don't mean the open access of the final research writeups such as open access journals, which is a topic of another blog post, but instead the tools, and services, used to access data from high impact papers to allow for one to reproduce results and/or building on top of existing literature. To put it in a metaphor, if a new research paper is a car, researchers don't want to see the showroom model but would rather pop the hood, understand the engine, and see what parts are upgrades from what they already have. </p> <p> Figure 1: Evolution of Open-Access generations showing the progression from basic data sharing to standardized formats, and finally to cloud-optimized real-time access. </p>"},{"location":"blog/the-technical-side-of-agu-2024-what-happened-and-where-are-we-going/#why-call-it-open-access-30","title":"Why call it Open-Access 3.0?","text":"<p>Similar to Web 1.0, 2.0, and 3.0 ideas, you can label versioning on an idea to indicate substantial jumps in the goals/roadmap. </p>"},{"location":"blog/the-technical-side-of-agu-2024-what-happened-and-where-are-we-going/#reflection-on-open-access-10","title":"Reflection on Open-Access 1.0","text":"<p>The first generation of Open-Access brought about achievements such as the CAMELS dataset (Newman et al. 2014) and even Google Earth Engine (GEE) (Gorelick et al. 2017) for providing services to subset spatial rasters. However, while the first generation of open-access science was predicated on data and code availability, there were flaws to the idea as there was not a universal standard for putting Data online with two exceptions: 1) There was a license for terms of usage, and 2) There would be some instructions on how to use the data to your benefit. While science has benefitted from this practice, it became apparent that these two requirements were not enough to pass on valuable research as one would need to download the code via FTP (either through a website download link or via Google Drive, Microsoft OneDrive, Box, etc.), understand the naming conventions around the variable names and units, and build code to ingest the data and sample into their model. This process could need to be replicated for each data source, creating overhead costs for collaboration. </p>"},{"location":"blog/the-technical-side-of-agu-2024-what-happened-and-where-are-we-going/#reflection-on-open-access-20","title":"Reflection on Open-Access 2.0","text":"<p>Thus, the next generation of open-access (2.0) intended to solve these problems by standardizing data structures and datasets. An example of dataset standardization is the FAIR guiding principles for scientific data management to improve the Findability,\u00a0Accessibility,\u00a0Interoperability, and\u00a0Reuse of digital assets (Wilkinson et al. 2016). Similar efforts were adopted in 2013 and again in 2020 to bring standards to model inputs/outputs through the Community Surface Dynamics Modeling System (CSDMS) Basic Modeling interface (BMI) (Peckham et al. 2013; Hutton et al. 2020). Recently, data structures have experienced standardizations such as the Python Array API and the Numpy (Harris et al. 2020) Array API to ensure the countless packages for scientific Python computing can be interoperable. While the increase in specifications helped reduce the time the user needs to take to understand the metadata/contents of a data repo and tools required for sampling the data, there was still two problems 1) Latency of downloads from Data hosting websites, and 2) speed/memory constraints with sampling methods (Even GEE, the most scalable of all Open-Access applications, has a page on coding best practices as certain operations can cause your runtime/compute to drastically increase). </p>"},{"location":"blog/the-technical-side-of-agu-2024-what-happened-and-where-are-we-going/#now-the-description-of-open-access-30","title":"Now the description of Open-Access 3.0","text":"<p>Thus, Open-Access 3.0  has the mission to develop near-real-time read/writes of scalable cloud observation, model, and research data in a manner that is accessible to all scientists. </p>"},{"location":"blog/the-technical-side-of-agu-2024-what-happened-and-where-are-we-going/#so-how-did-presentations-at-agu-support-open-access-30","title":"So, how did presentations at AGU support Open-Access 3.0?","text":"<p>While I believe I'm the first person to develop a semantic naming structure to describe the evolution of internet data availability in scientists (and if I'm not, feel free to let me know in the GitHub PR), I'm not the first person to recognize this trend. On Thursday, Joe Hamman of Earthmover gave an invited talk on his company's vision of \"Seamless Arrays\"\u2014the ability to easily query a data cube across spatial and temporal dimensions with low latency (Hamman et al. 2024). Some of the poster sessions focused on Open-Access 3.0 included work detailing toolboxes for watershed modeling (R\u00e9billout et al. 2024), methods for distributing array operations in a serverless fashion (White et al. 2024), and tools for retrieving files from an ID entry within a shared cloud registry (https://github.com/heliocloud-data/cloudcatalog). NOAA National Water Model (NWM) sessions included work detailing designing and implementing a BigQuery dataset and API for the NWM (Markert et al. 2024). Other works under the theme of open-access 3.0 include a poster on earth system modeling and benchmarking for CMIP6 data (Lee et al. 2024), and a talk on the icepyx software for querying ICESat-2 data.</p>"},{"location":"blog/the-technical-side-of-agu-2024-what-happened-and-where-are-we-going/#there-are-a-lot-of-similar-ideas-where-will-this-be-going-in-2025-should-i-get-in-on-open-access-30","title":"There are a lot of similar ideas... Where will this be going in 2025? Should I get in on Open-Access 3.0?","text":"<p>Given the past as a predictor of the future, I believe Open-Access 3.0 likely mirrors patterns we've seen in developer tools before. Consider the history of IDEs: while many groundbreaking early applications no longer exist, but their innovations laid the foundation for modern development environments. Some of the novel projects that exist today, like VIM and NeoVIM, were once the same project, but diverged at core points and now exist separately using different roadmaps. The projects often pull inspiration from one another, but are different entities. At its core, Open-Access 3.0 is straightforward: distributed reads and writes around cloud-optimized datasets. However, as we saw at AGU this year, this simple idea can become complex when datasets expand across spatial and temporal dimensions and increase in parameter count. Just as the IDE landscape eventually settled into a few dominant platforms while maintaining room for specialized tools, I believe Open-Access 3.0 will follow suit. Some platforms may emerge as standards, particularly for specific scientific domains or data types. However, the diversity of scientific computing needs means there will always be value in creating new approaches. If existing tools don't align with your research requirements\u2014whether it's for watershed modeling, ICESat-2 data querying, or machine learning I/O optimization\u2014the field needs your contribution. So dust off that keyboard and get working! AGU25 abstracts are due in 8 months!</p>"},{"location":"blog/the-technical-side-of-agu-2024-what-happened-and-where-are-we-going/#my-prediction-for-next-year","title":"My prediction for next year","text":"<p>Looking ahead to AGU 2025, I hope to see these tools mature and new methods emerge, particularly at the intersection of read/write services and machine learning. Machine learning samplers are built to handle big data. However, this data is often locally hosted rather than pulled from object storage in the cloud. I believe sampling directly through the object stores NOAA is putting together through their big data project on AWS (Simonson et al. 2022; Willett et al. 2023) and other publically available global data such as Caravan (Kratzert et al. 2023) on GCP will unlock the ability to use larger-scale models if scientists don't have access to high-powered computing. Further, given there is a push for openly sharing model checkpoint weights, I predict the combination of data-availability, and trained models, will usher in a surge of abstracts and funding for ML ventures.</p>"},{"location":"blog/the-technical-side-of-agu-2024-what-happened-and-where-are-we-going/#references","title":"References:","text":"<p>A. Newman; K. Sampson; M. P. Clark; A. Bock; R. J. Viger; D. Blodgett, 2014. A large-sample watershed-scale hydrometeorological dataset for the contiguous USA. Boulder, CO: UCAR/NCAR.\u00a0https://dx.doi.org/10.5065/D6MW2F4D</p> <p>Gorelick N, Hancher M, Dixon M, Ilyushchenko S, Thau D, Moore R. 2017. Google Earth Engine: Planetary-scale geospatial analysis for everyone. Remote Sensing of Environment. 202:18\u201327. doi:10/gddm6z.</p> <p>Hamman J, Abernathey RP, Cherian D. 2024. Seamless Arrays: A Full Stack, Cloud-Native Architecture for Fast, Scalable Data Access. AGU24.</p> <p>Harris, C. R., Millman, K. J., J., S., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., Van Kerkwijk, M. H., Brett, M., Haldane, A., Del R\u00edo, J. F., Wiebe, M., Peterson, P., . . .  Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), 357-362. https://doi.org/10.1038/s41586-020-2649-2</p> <p>Hutton et al., (2020). The Basic Model Interface 2.0: A standard interface for coupling numerical models in the geosciences. Journal of Open Source Software, 5(51), 2317, https://doi.org/10.21105/joss.02317</p> <p>Kratzert F, Nearing G, Addor N, Erickson T, Gauch M, Gilon O, Gudmundsson L, Hassidim A, Klotz D, Nevo S, et al. 2023. Caravan - A global community dataset for large-sample hydrology. Sci Data. 10(1):61. doi:10.1038/s41597-023-01975-w.</p> <p>Lee J, Ordonez AC, Ullrich P, Gleckler PJ, Dong B, Chang K, Durack PJ, Valkonen E, Caron J, Hur I, et al. 2024. An open-source Earth System Model evaluation and benchmarking tool: PCMDI Metrics Package (PMP). AGU24.</p> <p>Markert, K. N., Da Silva, G., Ames, D. P., Maghami, I., Williams, G. P., Nelson, E. J., Halgren, J., Patel, A., Santos, A., &amp; Ames, M. J. (2024). Design and implementation of a BigQuery dataset and application programmer interface (API) for the U.S. National Water Model. Environmental Modelling &amp; Software, 179, 106123. https://doi.org/10.1016/j.envsoft.2024.106123</p> <p>Peckham, S. D., Hutton, E. W., &amp; Norris, B. (2013). A component-based approach to integrated modeling in the geosciences: The design of CSDMS. Computers &amp; Geosciences, 53, 3-12. https://doi.org/10.1016/j.cageo.2012.04.002</p> <p>R\u00e9billout LR, Ozeren Y, Al-Hamdan MZ, Bingner R. 2024. PyAGNPS: A Python Toolbox for Watershed Modeling with AnnAGNPS. AGU24.</p> <p>Scheick J, Arendt AA, Beig M, Bisson K, Eidam E, Fair Z, Friesz A, Leong WJ, Piunno R, Snow T, et al. 2024. icepyx: community and software for the open science journey. AGU24.</p> <p>Simonson, A., Brown, O., Dissen, J., Kearns, E. J., Szura, K., &amp; Brannock, J. NOAA Open Data Dissemination (Formerly NOAA Big Data Project/Program). 65-94. https://doi.org/10.1002/9781119467557.ch4</p> <p>White T, Nicholas T, Abernathey RP. 2024. Cubed: Bounded-Memory Serverless Array Processing in Xarray. AGU24.</p> <p>Willett, D. S., Brannock, J., Dissen, J., Keown, P., Szura, K., Brown, O. B., &amp; Simonson, A. (2023). NOAA Open Data Dissemination: Petabyte-scale Earth system data in the cloud. Science Advances. https://doi.org/adh0032 </p> <p>Wilkinson MD, Dumontier M, Aalbersberg IjJ, Appleton G, Axton M, Baak A, Blomberg N, Boiten J-W, da Silva Santos LB, Bourne PE, et al. 2016. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data. 3(1):160018. doi:10.1038/sdata.2016.18.</p> <p> Share on </p>"},{"location":"blog/doing-ai-hydrology-to-assess-water-resources-for-ai-expansion-to-continue-doing-ai-hydrology/","title":"Doing AI Hydrology to assess water resources for AI expansion... to continue doing AI Hydrology","text":"<p>In January (2025), the Stargate project was announced by the president of the United States, which will be a $500 billion investment over the next four years building new AI infrastructure. </p> <p>In 1956, we introduced the National Interstate and Defense Highways Act (NIDHA) with an authorization of $25 billion (equivalent to $215 billion in 2024). The typical take on the NIDHA is that it facilitated an economic growth, national security and personal mobility. Another take, however, is that the highways destroyed American cities, segregated economic classes, and created a mobility barrier requiring a personal automobile. There is no doubt, what-so-ever, that the NIDHA changed American life and culture (for better or for worse...). Shortly after the NIDHA the air quality in Los Angeles reached an all time low, before the Clean Air Act in 1963 set a course for air quality improvement. Along with the Clean Water Act and the Endangered Species Act, these regulations are largely responsible for any protections we have against corporate pillaging of the environment.</p> <p>Part of the Stargate project is a commitment from the federal government to eliminate any barriers to the expansion of AI Infrastructure. Presumably, this means no need for environmental regulation, nor consideration of the impact to American lives or culture. </p> <p>Expanding AI Infrastructure will require increasing H2O demand, consumptive use, and pollution discharge, even with environmental regulation in place. The hydrological sciences now are thoroughly imbedded with AI Research. Common \"low hanging fruit\" papers are simply just slight modification of Deep Learning architectures, to keep up with the latest advancements from computer science, and adding a line on a standard benchmarking dataset. This is not by accident, but was actually called for by early adopters of the third book of AI in hydrology (including me).</p> <p>Use of AI tools, like LLMs, is rapidly on the rise. From a H20 standpoint, this fact is itself troublesome. Even though I find these tools incredibly useful for my daily tasks, particularly programming, I am now beginning to reflect on the importance of my task in general. I've been writing computer programs for hydrology and hydraulics modeling since 2008. Sometimes these models are considered \"AI\", sometimes they aren't, but recently almost all my models incorporate some aspect of AI. My whole academic career has been defined by my development and analysis of AI for hydrologic and hydraulics modeling. You see, I've been riding this growing wave of research funding specifically for AI for H&amp;H modeling. We are now at a point where hydro research without AI has little chance of funding. And I suspect this to be the case in most other academic fields as well. The feeling I am getting is \"Do AI, or we aren't going to fund your research\". </p> <p>Recently though, I've heard several anecdotes of businesses now taking the same stance. Startup companies that don't have much AI tech involved probably aren't going to get funded. Just yesterday I heard that one company that had a long standing client in MS, but didn't have an AI portfolio, was told by MS to \"start doing AI, or stop working with us.\" This goes well beyond a growth of a technology due to an organic increasing demand, and is starting to seem like growth for its own sake, perhaps as a means to keep the \"bubble\" growing. </p> <p>I've spend my entire life advocating for means of transportation other than personal automobiles. I didn't know why, I just liked walking, biking and skateboarding over sitting in a car. In my late teenage years, I figured out that I really enjoyed small streets, vs large streets, but didn't know why. I stumbled upon a blog/group call Strongtowns some point in my early 20s, and I figured out the why. Small streets feel more comfortable because they are designed at the \"human scale\", while large streets with high speed limits are uncomfortable because they are designed for automobiles which need more space, particularly when traveling at high speeds. From Strongtowns, I also learned about the Automobile Infrastructure Growth Ponzi Scheme. In short, we continue to develop new roads, and we neglect to maintain the roads we have. It is growth for its own sake, not for the benefit of travelers. In most cases, I would actually argue (feel free to ask me about this later), that expanding road networks actually inhibits travel. Automobile infrastructure growth happens to take a whole lot of H&amp;H modeling. </p> <p>I've now found myself in a scenario where I'll be using and developing AI-based hydrologic modeling in order to evaluate and plan for increasing H20 resources for AI expanding infrastructure. What a time to be alive!</p> <p> </p> <p>Note: This blog was written without the use of LLMs. Google was used sparingly.</p> <p> Share on </p>"},{"location":"blog/alphaearth-foundation-satellite-embeddings/","title":"AlphaEarth Foundation Satellite Embeddings","text":"<p>\u201cAlphaEarth Foundations provides a powerful new lens for understanding our planet by solving two major challenges: data overload and inconsistent information.\u201d \u2014 from the \u201cHow AlphaEarth Foundations Works\u201d section of the DeepMind post.</p> <p>The key word is lens. That is what this product appears to be.</p> <p>Before we look at the embeddings, let\u2019s look at the spatial foundation used to build them. Here is a yearly Red, Green, and Blue composite from Sentinel-2 L1C that matches the 10 m resolution of the AEF embeddings.</p> <p></p> <p>This image is what most people imagine when they look at satellite data. It\u2019s not far from a plane window view. When you first looked at it you likely noticed features relevant to your domain or places you know. Because of what you know, the image is worth more than its RGB components.</p> <p>Now to the AEF satellite embeddings. They were produced by a model that sought to replicate that kind of understanding by digesting an individually incomprehensible volume of information and distilling it into vectors that capture small amounts of human-interpretable structure. When we visualize different embeddings as if they were the R, G, and B channels of a composite image, the results are fascinating because recognizable phenomena emerge.</p> <p>However, with 64 unique embedding layers, checking every possible RGB combination (n = 41,664) would take time.</p> <p></p> <p>The Earth Engine Developers site has solid guides on using these embeddings for classic LULC and remote-sensing tasks. One clear use is similarity search and visual exploration, much like how people have used Google Earth, NAIP, and other high-resolution sources to curate training and validation datasets for models built on traditional EO inputs (e.g., Landsat and Sentinel).</p> <p>A caution: do not treat these embeddings as if they were native sensor bands. EO instruments are chosen for known physical relationships to real-world processes. Embeddings are powerful, but they are still a black-box representation. As relationships to biomass, crop yield, and other variables are reported, we should ground them in the physics and measurements that trained the model. Foundation models should strengthen the case for continuing programs like Landsat and for expanding EO, ground validation, and basic science. The more lenses we have, the more complete our understanding of Earth\u2019s condition and change.  </p> <p>AI Disclaimer: Chatgpt/GPT-5 was used for editing this post and helping with generating some of the visualization pulled from Google Earth Engine. </p> <p> Share on </p>"},{"location":"blog/hydrology-is-flat-and-its-buckets-all-the-way-down/","title":"Hydrology is flat, and its buckets all the way down!","text":"<p>For some reason, much of my recent work keeps coming back to buckets, and re-thinking the conceptualization of natural hydrologic systems as buckets. I am generally sick of talking about buckets. I'm hoping that this post is my farewell to thinking about buckets, at least for a while.</p>"},{"location":"blog/hydrology-is-flat-and-its-buckets-all-the-way-down/#sir-edmond-leakybucket","title":"Sir Edmond Leakybucket","text":"<p>When I was first learning differential equations the professor told us a silly story about Sir Edmond LeakingBucket, some ol' timey English royal who had to drink his ale quickly because his ale bucket leaked. I went on to study hydrology, so I've had to think about Sir Edmond for the past fiteen years. I can't escape him. Sometimes he mixes two kinds of ales together, sometimes his ale bucket is more complicated or simpler, but he is always losing his ale. Poor guy. Sir Edmond and his bucket do two important things: 1) gives nice differential equation examples, but more importantly for hydrology 2) Leaking buckets are a primary conceptualization for hydrologic processes. </p> <p>A simple differential equation for the ale level in Sir Edmond's bucket is:</p> \\[ \\frac{dh}{dt} = -k \\sqrt{h} \\] <p>Where \\(h(t)\\) is the ale at time \\(t\\), k is a proportionality constant that governs the rate of outflow. Its solution through seperation of variables is:</p> \\[   h(t) = \\left(\\sqrt{h_0} - \\frac{k}{2}t\\right)^2 \\] <p>Where \\(h_0\\) is the initial ale level in the bucket at time \\(t = 0\\). This gives us the opportunity to track volumes of ale through this bucket, and match the fluxes from buckets with data collected on real-world hydrological systesm. This is, in a nutshell, the field of computational hydrology, we just need to dress up and add complications to this bucket, and off we go.  </p>"},{"location":"blog/hydrology-is-flat-and-its-buckets-all-the-way-down/#a-walk-through-the-watershed","title":"A walk through the watershed","text":"<p>Going back again to undergraduate, I was building my first watershed model (Frame, 2010). I spend weeks staring at a computer screen, writing code, processing data, and thinking through discretization of Sir Edmond's ale bucket. I went out to do some field work for a different project in the Carmel River Watershed, a valley I've been to a dozen times. But after spending so much time thinking about this watershed digitally, well really just a digital bucket representing the watershed, a sense of dred came over me as I looked down the Carmel Valley (Figured below). I was realizing how much my precious computer model was missing, seeing the errors in my assumptions, the variety of vegetation in areas my models represents as homogeneous, the property boundaries with drainage features. Nothing in the analog system seemed to match my digitization. There is overwhelming complexity in every watershed, river reach and hillslope, if our eyes are open.  </p> <p> </p> <p>But in hydrology, we consider representing a delayed flow of water as a bucket a type of physical conceptualization. And as a matter of fact, this goes back a long way to Instantaneous Unit Hydrograph (IUH). In my MS level hydrology class, we learned that the IUH from linear reservoirs turns into a beta distribution (Nash 1960). This is a type of theoretical physical understanding of hydrology through buckets.  </p> <p>The equation for the Instantaneous Unit Hydrograph (IUH) from linear reservoirs is:  </p> \\[ q(t) = \\frac{t^{n-1} e^{-t/k}}{k^n \\Gamma(n)} \\] <p>Where \\(q(t)\\) is the flow rate at time \\(t\\), k is the storage coefficient (related to the delay in each reservoir), \\(n\\) is the number of reservoirs (controls the shape of the hydrograph), \\(\\Gamma(n)\\) is the gamma function.  </p> <p>The physical intuition is 1) the \\( n \\) reservoirs represent sequential storage compartments (or \u201cbuckets\u201d) that delay and attenuate flow as water moves downstream, and 2) each bucket\u2019s outflow becomes the inflow for the next, creating a cascade effect that smooths and delays the hydrograph. Hydrologists aren't the only ones to take simple conceptualizations and ask them to simulate complicated physical systems. This is similar to the use of harmonic oscillators in physics.  </p>"},{"location":"blog/hydrology-is-flat-and-its-buckets-all-the-way-down/#harmonic-oscillators","title":"Harmonic oscillators","text":"<p>The ultraviolate catastrophy is the result of using harmonic oscillators as conceptual representations of physical systems in Classical Physics. The derevation of the Rayleigh-Jeans Radiation Law (see LibreTexts 2024) is based on these simple conceptual oscillators, but this leads to predicted energy densities that diverged at high frequencies, an issue experimentally inconsistent with observed data.  </p> <p> </p> <p>Image credit: HyperPhysics, \u00a91998-2020, Department of Physics and Astronomy, Georgia State University. </p> <p>Max Planck resolved the ultraviolet catastrophe in 1900 by fundamentally rethinking the energy distribution, quantizing the energy, allowing the thermodynamic limit to match experimental data, giving evidence that energy in the real world is quantum, rather than continuous. Let's take note of a few important pieces of this success story. In the classical case, the energy was poorly concieved, assuming continuous energy distribution. Physicists knew this because the theory, when taken to the thermodynamic limit, didn't match observation data. This gap between theory and experiment ultimately inspired a revolutionary shift in conceptualization, paving the way for modern quantum mechanics, and improving our understanding of the natural world.  </p>"},{"location":"blog/hydrology-is-flat-and-its-buckets-all-the-way-down/#taking-buckets-to-their-limit","title":"Taking buckets to their limit","text":"<p>The mathematical structure of a system of buckets flowing into each other is the correct physical model of one type of system only, and that is, of course, a system of buckets flowing into each other. However, applying this model to a watershed is an approximation, much like using harmonic oscillators to model blackbody radiation. While it works under certain conditions, we know it\u2019s not a true representation of how the natural world behaves.  </p> <p>But let's take this idea of buckets flowing into each other a bit more seriously as a representation of a fundamental unit process of hydrology. When I am out on Carmel Valley Road overlooking the watershed, I see depressions, gullies, rills, blades of grass, everything else. All these individual processes do somewhat behave as oddly shaped buckets. A blad of grass, for instance, does collect moisture from the air, and that moisture does flow down the blade. A prarie pothole also fills up and overflows, bucket-like, yet some water flows down through the leaking bottom. Though these are not perfectly round reservoirs with known leaks with known coefficients, they do all sort of flow into each other. I used this idea to try to bring neural networks into hydrology from a more realistic conceptualization. With differentiable modeling (Shen et al., 2023), we can set up our neural network to behave like these systems of buckets, complete with valves on the spigots determining the flow coefficient (weights), and the water level dropping below the spigot shutting off flow completely (activation). What I think we are left with here is a pretty darn good digital interpretation of a messy, complex, heterogeneous system (image below from Frame et al., 2024).  </p> <p></p> <p>Each individual bucket in our Nash Cascade Neural Network behaves like a mass concerving perceptron (Wang et al., 2024). In this scenario, we end up with a model that has the uncanney ability to match a downstream diagnostic variable very well, and we have utilized the hydrologists tool of choice. But if we look closely, are we not simply re-creating a neural network? And if so, can we re-think the neural network as a conceptual model?  </p> <p>We've seen overwhelming evidence in the past few decades that neural networks are more accurate at predicting hydrologic responses than the conceptualization of hydrologic systems as buckets (Nearing et al., 2020). The predictive power of bucket-based conceptualizations can themselves be enhanced through integration with neural networks (Shen et al., 2023). So much, in fact, the neural network is more important than the bucket (Acu\u00f1a Espinoza et al., 2024). What I've always pondered is what would happen if hydrologists simply rejected conceptualizations that are demonstratively poor predictors (i.e., rejected hypotheses; Beven 2018 and 2019), and in their place we embrace the architectures leading to better predictions when compared observations (e.g., neural networks) as a new conceptualization. There is often a sentiment that the bucket conceptualizations, even as simplifications, are \"interpretable\", but an interpretation is only usefull if we are honest about them limitations. We still learn the Rayleigh\u2013Jeans law in thermodynamics, but we then understand the limitations of the continuous energy conceptualization and turn to Planck's Law to grasp the quantum reality. We can still learn bucket conceptualizations, but let's not pretend that they are \"physical\" representations. So far we've failed to reject neural networks as hydrology models, can we build our hydrologic conceptualizations around these architectures?  </p>"},{"location":"blog/hydrology-is-flat-and-its-buckets-all-the-way-down/#buckets-as-toy-models-as-an-educational-tool-and-for-hypothesis-testing","title":"Buckets as toy models, as an educational tool, and for hypothesis testing","text":"<p>One thing we can do with our digital buckets is use these simple concepts to explore complex ideas. Since we can easily generate synthetic data and test scenarios (Frame et al., 2023), we can study hydrological systems in a structured way. The Deep Bucket Lab models a \u2018leaking bucket\u2019 system to represent hydrological processes. Using synthetic data generated through numerical simulations, it demonstrates how factors like precipitation and bucket characteristics influence water flow dynamics. The lab employs a Long Short-Term Memory (LSTM) network to predict water levels and fluxes based on the synthetic data generated from the \u2018leaking bucket\u2019 model. Users can modify the model\u2019s parameters and experiment with different scenarios to explore hydrological process representations and their predictability. Interactive graphs and experiments allow for practical engagement with the concepts. The lab provides a straightforward way to study hydrological systems and apply machine learning techniques to hydrology.  </p> <p>But perhaps, developing educational material off of faulty conceptualizations further perpetuates the misconception, and we could do better by avoiding the analogy altogether. Instead, we might focus on directly engaging with the underlying physical principles and mathematical descriptions of hydrological systems. By prioritizing frameworks that capture the complexities of real-world processes, such as conservation laws and scale-dependent behaviors, we can provide learners with a more accurate and transferable understanding of hydrological dynamics. This approach encourages critical thinking and a deeper appreciation of the challenges in modeling complex environmental systems.</p>"},{"location":"blog/hydrology-is-flat-and-its-buckets-all-the-way-down/#buckets-all-the-way-down","title":"Buckets all the way down","text":"<p>It has been a bit of a joke between me and my hydrology friends (shout out to the editorial board of this blog!) that everything just becomes a bucket, if you think about it long enough. There are a few analogies that we can use to represent many processes in the naturual world. Buckets and harmonic oscillators are just a two examples. They are ubiquitus with dynamic systems. They represent a convenient unit that we can manipulate and modify to study complex behaviors in an otherwise overwhelming system. When we boil systems down to these analogies, we are playing around with ideas to help make us understand. We'll probably never know how the world actually works, we just don't have the sensory capabilities. Conceptualizations like buckets, oscillators, or reservoirs, help us bridge the gap between abstract mathematics and tangible phenomena. These models allow us to test theories, communicate ideas, and develop computational tools that extend our understanding of the natural world.</p> <p>The real beauty lies in the interplay between abstraction and application. While we recognize the limitations of these analogies\u2014no bucket, spring, or reservoir can capture the full complexity of a river basin or a turbulent atmosphere\u2014they still provide an essential scaffold for exploration. From conceptual hydrological models to modern machine learning architectures, these analogies offer a common language to connect diverse fields of study, sparking innovation in ways that would be impossible without simplification.  </p>"},{"location":"blog/hydrology-is-flat-and-its-buckets-all-the-way-down/#references","title":"References","text":"<p>Acu\u00f1a Espinoza, E., Loritz, R., \u00c1lvarez Chaves, M., B\u00e4uerle, N., and Ehret, U. (2024) To bucket or not to bucket? Analyzing the performance and interpretability of hybrid hydrological models with dynamic parameterization, Hydrol. Earth Syst. Sci., 28, 2705\u20132719, https://doi.org/10.5194/hess-28-2705-2024. </p> <p>Beven, K. J. (2018). On hypothesis testing in hydrology: Why falsification of models is still a really good idea. Wiley Interdisciplinary Reviews: Water, 5(3), e1278.</p> <p>Beven K. (2019) Towards a methodology for testing models as hypotheses in the inexact sciences. Proc. R. Soc. A 475: 20180862. http://dx.doi.org/10.1098/rspa.2018.0862</p> <p>Frame, J. M. (2010). An integrated surface water-groundwater interaction model for the Carmel River. https://digitalcommons.csumb.edu/caps_thes/36/</p> <p>Frame J. M., L. Hernandez Rodriguez, and M. Bassiouni (2023). \"DeepBucketLab - A Playground for Understanding Deep Learning for Hydrologic Process Representations,\" DOI: 10.5281/zenodo.14538195.</p> <p>Frame J. M., Bindas T., Araki R., Rapp J. and Deardorff E. (2024) Synchronization in hydrologic  processes and modeling the response with concepts, physics and neural networks. ESS Open Archive. DOI: 10.22541/essoar.171320241.14125931/v1</p> <p>LibreTexts. (Retrieved 2024). Deriving the Rayleigh-Jeans Radiation Law. Retrieved from https://chem.libretexts.org/Bookshelves/</p> <p>Nash, J. E., &amp; HRS. (1960). A unit hydrograph study, with particular reference to British catchments. Proceedings of the Institution of Civil Engineers, 17(3), 249-282. DOI: 10.1680/iicep.1960.11649</p> <p>Nearing, G. S., Kratzert, F., Sampson, A. K., Pelissier, C. S., Klotz, D., Frame, J. M., Prieto, C., &amp; Gupta, H. v. (2020). What Role Does Hydrological Science Play in the Age of Machine Learning? Water Resources Research. DOI: 10.1029/2020wr028091</p> <p>Shen, C., Appling, A. P., Gentine, P., Bandai, T., Gupta, H., Tartakovsky, A., ... &amp; Lawson, K. (2023). Differentiable modelling to unify machine learning and physical models for geosciences. Nature Reviews Earth &amp; Environment, 4(8), 552-567. DOI: 10.1038/s43017-023-00450-9</p> <p>Wang, Y. H., &amp; Gupta, H. V. (2024). Towards interpretable physical\u2010conceptual catchment\u2010scale hydrological modeling using the mass\u2010conserving\u2010perceptron. Water Resources Research, 60(10), e2024WR037224. DOI: 10.1029/2024WR037224</p> <p> Share on </p>"},{"location":"blog/welcome-to-deepgroundwater/","title":"Welcome to DeepGroundwater","text":"<p>Thinking profoundly</p> <p>and sharing code with the world</p> <p>are valued actions</p> <p> Share on </p>"},{"location":"blog/what-does-your-model-do/","title":"What does your model do?","text":"<p>In Earth Sciences we build all sorts of models for one reason or another. I've recently gotten into several conversations where we discuss an aspect of modeling that breaks down what we want, what we ask for, and what the model was actually built to do. In an ideal scenario, all three of these would be the same. But, in reality, this is never the case. We need to build models sometimes only as a stepping stone to our desired goal. Sometimes that stepping stone has to be shakey in order to get to where we want to go. But one important thing about modeling, is the model is NEVER the end goal, or at least it shouldn't be. I worry sometimes that models are being build for their own sake. If you are having a hard time connecting the dots of how your model will be used, this blog is for you. </p> <p>We can visualize this problem as a 3D coordinate system. On the X-axis, we have how well-defined our task is. On the Y-axis, we have how achievable it is. We might have a Z-axis: Alignment. If the circles of our intention don't overlap, we end up successfully completing a task that fails to solve the actual problem. Let's break every modeling project into three distinct items:</p> <ul> <li>Goal: What do you actually want?</li> <li>Task: What did you ask the model to do?</li> <li>Design: What was the model designed to do?</li> </ul> <p>When these three are in direct overlap, you have a useful tool. When they drift apart, you have a \"Success-Failure\": a model that does exactly what you told it to, while not bringing you any closer to your end goal.</p>"},{"location":"blog/what-does-your-model-do/#example-1-should-i-bring-my-umbrella","title":"Example 1: Should I bring my umbrella?","text":"<p>This is an example of an ideal alignment of goal, task and design in modeling. You want to stay dry on your walk to work. You use a short-range weather model to see if it will rain in the next hour.</p> <ul> <li>Goal: Bring umbrella on walk.</li> <li>Task: Forecast weather 1 hour out.</li> <li>Design: Short-range forecasting.</li> </ul> <p>In this scenario, the definitions are precise and the task is highly achievable. The circles overlap almost perfectly. If the model says \"rain,\" and you grab the umbrella, your goal is met. The end goal of bringing an umbrella is relatively small compared to the massive modeling frameworks that go into modern weather forecasting.</p> <p> </p>"},{"location":"blog/what-does-your-model-do/#example-2-do-i-need-a-jacket-on-my-trip-next-week","title":"Example 2: Do I need a jacket on my trip next week?","text":"<p>Now, let\u2019s stretch the temporal scale. You\u2019re packing for a trip next week. You ask a mid-range model for a forecast.</p> <ul> <li>Goal: Pack for upcoming trip.</li> <li>Task: Forecast weather 1 week out.</li> <li>Design: Mid-range weather forecasting.</li> </ul> <p>Here, the Definition is still high, but Achievability starts to drop. Chaotic atmospheric dynamics mean that a 7-day forecast is less reliable than a 1-hour one. The overlap between your Task and your Goal shrinks. You might pack shorts based on a \"Sunny\" forecast, only to arrive in a cold snap. The model did its \"Task,\" but the \"Goal\" (being prepared) was undermined by the inherent limits of the \"Design.\"</p> <p> </p>"},{"location":"blog/what-does-your-model-do/#example-3-solving-flooding-the-alignment-trap","title":"Example 3: Solving Flooding (The Alignment Trap)","text":"<p>This is where things get dangerous. We often set out with a massive, virtuous goal, such as eliminate flood hazards. But how do we translate that into a computational task? Sometimes, we produce a model, validate the results, then call it a day, even though this won't neccessarily get us closer to our goal.</p> <ul> <li>Goal: Eliminate flood hazards.</li> <li>Task: Produce 100-yr flood map.</li> <li>Design: Inundation from streamflow.</li> </ul> <p>The Task is extremely well-defined (X-axis) and generally achievable (Y-axis). But the Alignment with the Goal is poor. A 100-year flood map tells you where the water goes in a massive, rare event. It doesn't tell you about the 20-year flood that washes out a low-water crossing, or the flash flood that overwhelms a storm drain. The end goal of solving flood hazards entirely is much larger than even the most complex models that go into flood prediction.</p> <p>If we use the map as our sole development strategy, we might \"pass\" our task by keeping houses out of the 100-year zone, but \"fail\" our goal because people are still dying in their cars on \"safe\" roads.</p> <p> </p> <p>We should strive for as much overlap as possible between our end goal, model design, and model task. This is extremely hard, however. We should at least be transparent about our task alignment. We can continue improving our models to give us better \"Tasks\" (better hydrograph matching), but if our \"Goal\" is ecosystem health or water equity, we can't assume that a better-fitted line on a graph automatically translates to a better outcome in the real world.</p> <p>I recommend that next time you are working on a model, you think through this alignment excercize.</p> <p> Share on </p>"},{"location":"blog/coding-blog-how-to-access-nwm-forcings-using-virtualizarr/","title":"Coding Blog: How to access NWM forcings using VirtualiZarr","text":"<p>One of the most underrated aspects of NOAA's weather data is how much data is published on a daily basis. The National Water Model (Cosgrove et al. 2024) produces nine total operational configurations, each with a different look back peroid, forecast range, and domain extent. The number of .nc files output to S3 Object Storage is in the tens of thousands... per day!</p> <p>While this amount of data is monumental for machine learning, or other hydrological analysis, it's cumbersome to read every .nc file individually, or download this amount of data to disk. That is where VirtualiZarr comes into play as it allows existing .nc files / structures to be viewed as zarr stores/xarray datasets without having to duplicate any data!</p> <p>Below is a tutorial on how you can use VirtualiZarr to read a forecast from the National Water Model as a singular zarr store for your own studies</p>"},{"location":"blog/coding-blog-how-to-access-nwm-forcings-using-virtualizarr/#installing-dependencies","title":"Installing dependencies","text":"<p>The following dependencies will be used, with a version of Python &gt;=3.11 <pre><code>dask==2025.2.0\ndistributed==2025.2.0\nxarray==2025.1.2\ns3fs==2025.2.0\nvirtualizarr==1.3.1\nh5py==3.13.0\ncubed-xarray==0.0.7\nh5netcdf==1.5.0\n</code></pre></p> <p>You can either put these into a <code>requirements.txt</code> file, or install them one by one via your package manager.</p>"},{"location":"blog/coding-blog-how-to-access-nwm-forcings-using-virtualizarr/#figure-out-what-data-youll-be-using-from-the-national-water-model","title":"Figure out what data you'll be using from the National Water Model","text":"<p>Given how much data exists out there from the national water model, you'll have to be explicit as to what you're trying to access. You can check out the National Water Model Bucket yourself, but the following information needs to be known:</p> <ul> <li><code>date</code><ul> <li>The datetime we're reading</li> <li>ex: <code>20250216</code></li> </ul> </li> <li><code>forecast_type</code><ul> <li>The forecast issued by the National Water Model</li> <li>ex: <code>short_range</code></li> </ul> </li> <li><code>initial_time</code><ul> <li>The time at which the forecast was issued (In Zulu time)</li> <li>ex: <code>t00z</code></li> </ul> </li> <li><code>variable</code><ul> <li>The variable we're looking to read</li> <li>ex: <code>channel_rt</code></li> </ul> </li> </ul>"},{"location":"blog/coding-blog-how-to-access-nwm-forcings-using-virtualizarr/#query-your-files","title":"Query your files","text":"<p>Since this data reads from the cloud, we'll need to create our paths to s3. Below shows how we'll be creating the URLs to our datasets dynamically.</p> <pre><code>import fsspec\n\nfs = fsspec.filesystem(\"s3\", anon=True)\nfile_pattern = f\"s3://noaa-nwm-pds/nwm.{date}/{forecast_type}/nwm.{initial_time}.{forecast_type}.{variable}.*.nc\"\nnoaa_files = fs.glob(file_pattern)\nnoaa_files = sorted([\"s3://\" + f for f in noaa_files])\n\nso = dict(anon=True, default_fill_cache=False, default_cache_type=\"none\")\n</code></pre>"},{"location":"blog/coding-blog-how-to-access-nwm-forcings-using-virtualizarr/#read-your-stores-from-the-cloud","title":"Read your stores from the cloud","text":"<p>Now that our s3 links are created, let's spin up a Dask Cluster to read our files from the cloud in parallel</p> <pre><code>import xarray as xr\nfrom virtualizarr import open_virtual_dataset\nfrom dask.distributed import LocalCluster\n\ndef _process(url: str, so: dict[str, str]):\n    vds = open_virtual_dataset(\n            url, \n            drop_variables=\"crs\",\n            indexes={}, \n            reader_options={\"storage_options\": so}\n        )\n    return vds\n\nfs = fsspec.filesystem(\"s3\", anon=True)\nclient_settings: dict[str, int | str] = {\n    \"n_workers\":9,\n    \"memory_limit\":\"2GiB\",\n}\n\ncluster= LocalCluster(**client_settings)  \nclient = cluster.get_client()\n\nso = dict(anon=True, default_fill_cache=False, default_cache_type=\"none\")\n\nfutures = []\nfor url in noaa_files:  \n    future = client.submit(_process, url, so)\n    futures.append(future)\n\nvirtual_datasets = client.gather(futures)  \n</code></pre> <p>Notice: How we drop the CRS from these netcdf files as their string byte-type throws an error</p>"},{"location":"blog/coding-blog-how-to-access-nwm-forcings-using-virtualizarr/#build-your-virtualizarr-store","title":"Build your VirtualiZarr store","text":"<p>Now that our virtual datasets are created, let's create the virtual reference and save the output locally to a kerchunk json file.</p> <pre><code>virtual_datasets[0].virtualize.to_kerchunk(\"NWM.json\", format=\"json\")\n</code></pre> <p>Outputs can be read through thefollowing: <pre><code>import s3fs\nimport os\n\nos.environ['AWS_REGION'] = 'us-east-1'\nfs = s3fs.S3FileSystem(anon=True, client_kwargs={'region_name': 'us-east-1'})\nstorage_options = dict(\n    remote_protocol=\"s3\", remote_options=dict(anon=True)\n)\nds = xr.open_dataset(\n    \"NWM_ACCET.json\",\n    engine=\"kerchunk\",\n    backend_kwargs={\"storage_options\": storage_options},\n)\nds.ACCET.plot(vmin=-1, vmax=ds.ACCET.max().values)\n</code></pre></p> <p> Figure 1: Plotted Accumlated total ET for CONUS </p> <p>That's it! You now how a collection of many .nc files in one xarray dataset that can be read into your hydrologic analysis.</p> <p>For the full demo, and code package we put together, check out our demo repo at: DeepGroundwater NWM Batcher. With a snippet below:</p> <pre><code>virtual_datasets = nwm_batcher.read(\n    date=\"20250516\",\n    forecast_type=\"short_range\",\n    initial_time=\"t00z\",\n    variable=\"land\",\n    data_variable=\"ACCET\",\n    coordinates=[\"time\", \"reference_time\", \"x\", \"y\"]\n)\n</code></pre> <p> Share on </p>"},{"location":"blog/routing-is-a-game-of-telephone/","title":"Routing is a Game of Telephone","text":""},{"location":"blog/routing-is-a-game-of-telephone/#what-game-are-we-playing","title":"What Game are we playing?","text":"<p>In Hydrology there are two types of models: Lumped and Distributed. Lumped modeling involves taking an area-based measurement within a specific boundary while distributed modeling takes individual predictions and disperses flow downstream. While there are too many lumped modeling methods to keep track of, there are only few distributed modeling methods that exist. All physics-based distributed modeling methods are different solutions to the continuity (mass conservation) and momentum (timing) equations, with Muskingum-Cunge (MC) being the most commonly used. MC is a center-in-time finite difference solution to the continuity equation, assuming a prismatic flood wave as the constitutive relationship to simplify the momentum equation, and predicts the amount of downstream discharge <code>Q(t+1)</code> given a summation of scaled-inflows and outflows <code>Q(t), I(t), I(t+1)</code> where the scalar terms to each input are determined through unitless terms calculated through channel geometries, friction, and the routing timestep. Deep Learning distributed methods exist via graph-neural networks, but are not commonly used (my assumption is because they are a new to hydrology). Graph Neural Networks work well for routing since a river can be represented as the edges of a graph, with the nodes being a junction between multiple rivers. Additionally, graph neural networks utilize message passing to apply a black-box weighted function between nodes. Physical distributed models are essentially message-passing graph networks whose functions respect physical laws!</p> <p>Almost all of my hydrology career, thus far, has been entangled with scaling and speeding-up MC routing methods; however, I cannot get away from the idea that the way we do physics routing is not the right answer to the ungaged streamflow problem. Yes, the application of MC works for generating a value that is reasonable when compared to local predictions, but these physical models do not have a solution for biases contained within streamflow observations. No hydrological prediction has been shown to be perfect, thus there exists some amount of error within its time-series hydrograph. When unit-catchment flow predictions are summed across a basin to route ungaged flows, the error accumulates as well as there isn't a mechanism to \"forget\" what has been added to the system. When these predictions are included in a routing scheme, momentum allows for better timing, but rarely are leakance/loss terms imposed.</p> <p>To put this into simple terms, we're playing one giant game of telephone where the message (flow prediction) at the end of the game may/may not be correct due to a mistake along the way. The extreme case of this is shown in the below episode of Family Guy when our protagonist is asked to get a loaf of bread, a container of milk, and a stick of butter, but comes home with treasure from a fish tank and a Joe Dirt DVD. </p> <p></p> <p>Looking to computer science / information theory literature, the game of telephone is often referred to as temporal error accumulation, small prediction errors compounding over time as each timestep's output becomes the next timestep's input. However, this problem gets worse as for routing models there isn't one chain of information (one river), but often many rivers/tributaries flowing into the same confluence leading to spatial error accumulation. The classic game of telephone has become a telethon, where nearby river networks are playing many simultaneous telephone games, merging at every confluence, compounding errors in both space and time.</p> <p>I'll say it again, there has to be a better way.</p> <p>If the predictions from our model of choice can represent the catchment and it's total water mass inflows/outflows at the small scale, then we don't have to worry about routing problems. The reason I highlighted small scale is because over a large enough area, biases will still accumulate. The game of telephone can be won if there is perfect communication transfer, but given <code>t</code> timesteps across <code>n</code> nodes, biases will accumulate and cause problems.</p> <p>Also no. Physical models assume that flow is un-impeded, all parameters are accurate per river per timestep (channel parameters, roughness), and cannot account for biases introduced from outside of the realm of the equations. This includes human-made interventions such as locks, dams, reservoirs, pipes, sewers, etc. that cause biases in our models. The conservation of mass becomes a conversation of bias.</p>"},{"location":"blog/routing-is-a-game-of-telephone/#how-to-fix-the-telethon","title":"How to fix the \"Telethon\"?","text":"<p>NOAA-OWP maintains a system of regional forecasting centers, and gages, across CONUS to provide observations at determined locations, displaying the results through the water.noaa.gov web portal. By using observations at key points along large river networks, total flow can be reset, detaching networks and shrinking the game of telephone into smaller pieces. Specific sections of CONUS are covered by the Replace and Route program: A methodology that ingests the official streamflow forecasts issued by the NWS RFCs at AHPS gauge locations, and utilizes the National Water Model (NWM) routing algorithm to propagate flow downstream. This method is used to delineate the River Forecast Center Flood Inundation Map (RFC FIM). </p> <p> </p> <p>While this approach removes error, it is costly and requires in-situ infrastucture plus modelers to be monitoring the system. River systems can change, which means errors and biases may have to adjust; causing a change in the location or monitoring strategy for a part of the graph. </p> <p>One can also turn to the world of machine learning / differentiable modeling for an answer. Differentiable routing models can learn river parameters to assist with timing, or calculate water loss per river reach. However, parameter learning alone doesn't solve the fundamental issue: the physics faithfully routes whatever you give it, biases and all. There needs to be an additional term to account for specific biases. NeuralOGCM offers a potential a hybrid architecture where a differentiable physics core handles large-scale dynamics while a neural corrector learns to fix what the physics gets wrong: <pre><code>Q_final = Q_physics + Q_correction\n</code></pre> This blog post is a call to arms to create a better solution. What the modeling community needs is DL-based routing schemes that can learn to correct systematic biases at specific points in the spatiotemporal river graph and learn where to apply corrections. This means approaches that can learn spatial patterns of bias from regions of the river graph, adjust internal states to correct for said biases, and break connectivity / mass-conservation when appropriate. The game of telephone doesn't have to end with a Joe Dirt DVD, but can instead stay on track and advance predictions in ungaged locations.</p> <p> Share on </p>"},{"location":"blog/soils-animated-part-1/","title":"Soils Animated: Part 1","text":"<p>Animation is a powerful tool for demonstrating scientific concepts. It is engaging, simplifies abstract ideas, and makes them accessible to a wide audience.</p> <p>In hydrology, one of the most simple yet elegant conceptualization of soil-water dynamics is the soil moisture loss function, a model developed by Laio et al. (2001) and Rodriguez-Iturbe et al. (1999). This model abstract complex soil processes at the field scale into a few key variables, providing ecohydrologists with a powerful 'toy model' for conducting a variety of interesting experiments. For example, Entekhabi and Rodriguez-Iturbe (1994) explored the impacts of spatio-temporal aggregation on characterizing heterogeneity of soil moisture dynamics. D'Odorico and Porporato (2004) used it to explain soil moisture seasonality.  </p> <p>In this blog post, I'll animate this soil dynamics model, with Part 1 focusing on the basic concepts.  Animations in this blog post will illustrate how hydrologists conceptualize the soil processes happening just above and beneath our feet,  while also exploring how these processes unfolds across different conceptual spaces. </p>"},{"location":"blog/soils-animated-part-1/#soil-moisture-loss-function-to-describe-the-dynamics","title":"Soil Moisture Loss Function to Describe the Dynamics","text":"<p>The soil moisture loss function (Laio et al., 2001);Rodriguez-Iturbe et al., 1999) relates the rate of loss from soil volume (\\(-\\frac{d\\theta}{dt}\\)) at a given soil moisture level (\\(\\theta\\)). It is a type of Storage-Flux relation model in Hydrology, linking how Fluxes (in this case, drainage and evapotranspiration) are regulated by Storage (soil moisture) availability. The model describes two key stages: Drainage and Evapotranspiration (note: For simplicity, hygroscopic water are neglected in this blog post).</p>"},{"location":"blog/soils-animated-part-1/#drainage-stage","title":"Drainage Stage","text":"<p>Imagine the soil after a rainstorm. Immediately after rainfall ceases, soil is saturated or extremely wet\u2014if you step on it, it is muddy and squishy. In this state, both soil macropores and micropores are filled with water, and water in the macropores drains quickly due to gravity. Some water may also run off the surface. The first stage of the loss function represents this state, and this rapid processes are expressed as an exponential function. </p> <p> Figure 1: Simulated soil moisture dynamics during the Drainage stage in two different conceptual spaces.  </p>"},{"location":"blog/soils-animated-part-1/#evapotranspiration-stage","title":"Evapotranspiration Stage","text":"<p>After a few days, the soil transitions to a semi-dry state\u2014it still feels moist to the touch, but no water seeps out. At this point, water in the soil macropores are mostly drained, and water is held against gravity in the soil micropores due to soil suction forces.</p> <p>In this stage, the dominant flux is evapotranspiration (ET). Initially, when the soil is wet, ET occurs at its maximum rate because plant stomata are fully, open, and the transpiration process reaches its maximum rate (note: though evaporation can continue independently; Krell et al., 2021). As the soil dries, a critical soil moisture threshold \\(\\theta^*\\) is reached, where plants begin to experience water stress. Below that point, ET decreases proportionally with soil moisture level (although this can be nonlinear; Araki et al., 2024). These ET dynamics are represented by the piecewise-linear function in the loss function space. This is much slower process compared to the Drainage, as you can see below: </p> <p> Figure 2: Simulated soil moisture dynamics during the Evapotranspiration stage in two different conceptual spaces.  </p>"},{"location":"blog/soils-animated-part-1/#usefulness-of-the-soil-moisture-loss-function","title":"Usefulness of the Soil Moisture Loss Function","text":"<p>This loss function model treats soil volume as a system responding to pulse inputs of rainfall (which I plan to elaborate on and animate in Part 2). The model describes how soil responds in a prescribed manner based on the combination of its physical properties, external forcing, and current  state. The drydown curve is the realization of the system partially activated by a rainfall pulse. </p> <p>Because the loss function is an ordinary differential equation (ODE), we can transition between two perspectives. </p> <ol> <li> <p>System Space (Left panels in the above animations): represents general patterns of the soil system, i.e., the loss function. </p> </li> <li> <p>Observable Space (Right panels in the above animations): describes soil moisture drying behavior, which are measurable with sensors. This drydown model is the analytical solution of the loss function ODE.</p> </li> </ol> <p>One of the biggest advantages of moving between these spaces is dealing with uncertainties in model estimation. </p> <p>In system space (loss function), both the x- and y-variables are subject to observation errors in \\(\\theta\\), making it prone to uncertainties. The animation below shows how Gaussian noise in the sample observation points  impacts loss estimates. This situation calls for bivariate analysis but they are more difficult to implement. One of the ways to deal with the bivariate uncertainties is to use the Bayesian approaches as implemented by Bassiouni et al., (2020).</p> <p> Figure 3: Estimating the soil moisture loss function from a sample point with Gaussian noise, from the loss function space.  </p> <p>In the observable space (which is the analytical solution of the loss function), the x-variable becomes sampling timing\u2014a deterministic quantity. This reduces uncertainty to just the y-variable (\\(\\theta\\)), allowing the application of the simpler nonlinear least-squares fitting.</p> <p> Figure 4: Estimating the soil moisture loss function from a sample point with Gaussian noise, from the drydown space.  </p>"},{"location":"blog/soils-animated-part-1/#the-power-of-the-animation","title":"The Power of the Animation","text":"<p>Science requires abstraction of processes, and we use models for that purpose. With the use of animation, we can easily visualize the model behavior and visualize more abstracted spaces. Animation helps us to wrap around our head to move between observation space and theoretical spaces, just like the drydown space and the loss function space. </p> <p>Platforms like DeepGroundwater enable us to share these visualizations (a shameless plug)! While animations are difficult to include in traditional scientific formats like PDFs, this platform freely accommodates images, animations, videos, and even JavaScript animations to share ideas. We welcome scientific &amp; artistic contributions here.</p> <p>Part 2 of this blog post will explore the challenges of using this soil moisture loss function model\u2014which is ultimately a bucket model that is an oversimplification of the reality as pointed out in our first blogpost. I plan to describe how loss function plays out in longer timeseries with multiple pulse rainfall pulses, and discuss how we may or may not be able to derive information from it (with animations, of course!). Stay tuned.</p>"},{"location":"blog/soils-animated-part-1/#code-availability","title":"Code Availability","text":"<p>These animations come to life thanks to Python's matplotlib, which you can access the code here: https://github.com/RY4GIT/drydown-viz.</p>"},{"location":"blog/soils-animated-part-1/#reference-acknolwegements","title":"Reference &amp; Acknolwegements","text":"<p>This blog post builds on ideas from discussions with my advisors and collaborators, especially Hilary, Kelly, Bryn, and the Editorial Board members\u2014thank you all!</p> <p>Laio, F., Porporato, A., Fernandez-Illescas, C. P., &amp; Rodriguez-Iturbe, I. (2001). Plants in water-controlled ecosystems: Active role in hydrologic processes and responce to water stress IV. Discussion of real cases. Advances in Water Resources, 24(7), 745\u2013762. https://doi.org/10.1016/S0309-1708(01)00007-0</p> <p>Rodriguez-Iturbe, I., Porporato, A., Ridolfi, L., Isham, V., &amp; Coxi, D. R. (1999). Probabilistic modelling of water balance at a point: the role of climate, soil and vegetation. Proceedings of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences. https://doi.org/10.1098/rspa.1999.0477</p> <p>D'Odorico, P., &amp; Porporato, A. (2004). Preferential states in soil moisture and climate dynamics. Proceedings of the National Academy of Sciences of the United States of America, 101(24), 8848\u20138851. https://doi.org/10.1073/pnas.0401428101</p> <p>Entekhabi, D., &amp; Rodriguez-Iturbe, I. (1994). Analytical framework for the characterization of the space-time variability of soil moisture. Advances in Water Resources, 17(1), 35\u201345. https://doi.org/10.1016/0309-1708(94)90022-1</p> <p>Krell, N. T., Morgan, B. E., Gower, D., &amp; Caylor, K. K. (2021). Consequences of dryland maize planting decisions under increased seasonal rainfall variability. Water Resources Research, 57(9). https://doi.org/10.1029/2020wr029362</p> <p>Araki, R., Morgan, B., McMillan, H. K., &amp; Caylor, K. (2024). Nonlinear soil moisture loss function reveals vegetation responses to water availability. Authorea Preprints. https://doi.org/10.22541/essoar.172251989.99347091</p> <p>Bassiouni, M., Higgins, C. W., Still, C. J., &amp; Good, S. P. (2018). Probabilistic inference of ecohydrological parameters using observations from point to satellite scales. Hydrology and Earth System Sciences, 22(6), 3229\u20133243. https://doi.org/10.5194/hess-22-3229-2018</p> <p> Share on </p>"},{"location":"home/contributions/","title":"How to contribute","text":"<p>Below are the steps needed to create a blog post for DeepGroundwater:</p> <ol> <li>Fork the DeepGroundwater github website</li> <li> <p>Run the following commands to install mkdocs to your env</p> <pre><code>pip install uv\nuv venv\nsource .venv/bin/activate\nuv pip install -e .\n</code></pre> </li> <li> <p>Create a .md file in the <code>docs/blog/posts</code> folder </p> </li> <li>Follow the following format for your post to ensure it builds on MkDocs     <pre><code>---\ndate:\ncreated: YYYY-MM-DD\nauthors:\n- your_author_username\n---\n\n# Title\n\nHere is where your header (abstract) will be. This is what will appear on the blog post site\n\n&lt;!-- more --&gt; # This is here to mark where the header ends and the content begins\n\nBlog post content!\n</code></pre></li> <li>Add your name to the <code>docs/blog/.authors.yml</code> file, following the format of the page</li> <li>Run <code>mkdocs serve</code> in your terminal using the new env to test your blog post renders. Check for formatting</li> <li>Push your code to your fork, and create a pull request!</li> <li>In your pull request, select any of the editorial board to create your review. Two are required for the post to go live</li> <li>Once the post is reviewed, we'll merge your PR!</li> </ol>"},{"location":"roadmap/","title":"Roadmap","text":"<p>DeepGroundwater is looking to expand out of Beta development, and into a consistently-updated blog with  articles from both the Editoral board members, and scientific community. </p>"},{"location":"roadmap/#features","title":"Features","text":"<p>Below are ideas we are pursuing at DeepGroundwater to make the user, and research sharing, experience better:</p> <ul> <li> Zenodo API Tools</li> <li> arXiv API Tools</li> <li> s3 API Tools </li> <li> Dockerization tutorials for complex apps</li> <li> Reference materials for storing dataset information</li> </ul> <p>If you're interested in tackling these ideas, feel free to open an issue in our GitHub page.</p>"},{"location":"blog/archive/01/01/2026/","title":"01/01/2026","text":""},{"location":"blog/archive/12/29/2025/","title":"12/29/2025","text":""},{"location":"blog/archive/08/03/2025/","title":"08/03/2025","text":""},{"location":"blog/archive/07/31/2025/","title":"07/31/2025","text":""},{"location":"blog/archive/02/21/2025/","title":"02/21/2025","text":""},{"location":"blog/archive/01/12/2025/","title":"01/12/2025","text":""},{"location":"blog/archive/12/22/2024/","title":"12/22/2024","text":""},{"location":"blog/archive/12/20/2024/","title":"12/20/2024","text":""},{"location":"blog/archive/11/28/2024/","title":"11/28/2024","text":""},{"location":"blog/author/taddyb/","title":"Tadd Bindas","text":""},{"location":"blog/author/jmframe/","title":"Jonathan Frame","text":""},{"location":"blog/author/rappjer1/","title":"Jeremy Rapp","text":""},{"location":"blog/author/RY4GIT/","title":"Ryoko Araki","text":""}]}